\subsection{Benchmarking}

Explain the different metrics that will be used for the experiments.

The first metric that will be measured for this benchmark is the total number of source lines of code.
This will be used to see whether an agent system implemented in Elixir is more concise than a system implemented using JADE\@.
If Elixir is indeed more concise than JADE it would throw into question whether using a framework for implementing multi-agent systems is worth it.
Having a more concise project is beneficial as the less lines of code there are, the more maintainable it is and the easier it is to find bugs.

The command line application cloc (Count Lines of Code)\footnote{\url{https://github.com/AlDanial/cloc}} will be used to count the number of source code lines in each project.
An external application will be used to count the number of lines due to the possibility of human error when counting lines in potentially dozens of files.
The version of cloc that will be used is \verb|1.82|.

\Cref{lst:cloc} shows the output of cloc when run against the directory containing the example projects for \cref{sec:comparison}.
The arguments passed to cloc are \verb|--vcs=git| which tells it to use git to list the files in a directory, \verb|--hide-rate| which makes the output of cloc deterministic, and the directory name containing the source code files that need to be counted.
The benefit of indicating that git is the version control system that is being used is that it prevents cloc from counted the lines of non-versioned files.
This prevents build artefacts from being included in the line count.

\begin{lstlisting}[numbers=none,float=h,label=lst:cloc,caption=Output of cloc when run on the example projects]
cloc --vcs=git --hide-rate dissertation/examples
      34 text files.
      34 unique files.
      14 files ignored.

github.com/AlDanial/cloc v 1.82
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Java                             2             26              0             96
Maven                            2             10              0             92
Elixir                           4             19              0             92
Erlang                           4             31              0             81
Python                           3             17              0             79
Markdown                         5             19              0             51
-------------------------------------------------------------------------------
SUM:                            20            122              0            491
-------------------------------------------------------------------------------
\end{lstlisting}

The output of cloc is a table containing columns for each language that is used in the directory, along with the number files of that language, how many blank lines there are, how many comments there are and the actual lines of source code.
For the purpose of this benchmark only the ``code'' column is relevant as this is the number of lines that contain program code.

The directories that will be measured for the two projects will be \verb|src/|\\ \verb|supply_chain_elixir/lib| and \verb|src/supply_chain_jade/src/main/java|.
These directories have been chosen as they only contain the source code for the application instead of also holding configuration files which would unfairly increase the line count.

CPU usage.

One of the ways to measure the speed of the system is to measure the time it takes in order to complete all 220 rounds of the simulation.
This would show the actual speed of the multi-agent system instead of just how much CPU usage it uses.

Both the Elixir and JADE implementation print log messages each time a new simulation round starts.
By looking at when the final round message is printed and subtracting the time when the first round message was printed, we get the total runtime for the simulation.

As a message is printed for each round, it is also possible to work out the time between each round.
This would allow us to calculate the average round time as well as the minimum and maximum round times.
This would be useful to see if each round completes in roughly the same amount of time or whether there are rounds that take significantly longer.

The log messages are printed with millisecond precision time which should be enough precision for this benchmark.
It would be possible to get nanosecond precision for the Elixir system by calling the Erlang standard library time functions and printing that separately.
Unfortunately, nanosecond precision would not be possible for the JADE system.
This feature was only made available in JDK 9 onwards, while this project is using JDK 8.

Memory usage.
