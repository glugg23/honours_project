\section{Methodology}

This section covers the design of the system as well as how it will be benchmarked.

\subsection{Implementation}

This section will introduce some of the common details about both of the implementations.
Such as how the agent will be designed in general and how each agent will communicate with each other.

Docker will be used to test the distributed system.
This is because it provides a fast way of starting multiple instances at once.

\subsubsection{Elixir}

Each of the parts of the agent will be using an OTP supervisor to manage the processes for that part.
Each agent will share the majority of the code to avoid duplicated code.
Then at start time, the correct behaviour for each agent will be enabled according to configurations files or environment variables.

Each individual agent will run on a separate node which will be\\auto-discovered using the `libcluster' library.

\subsubsection{JADE}

Not yet sure how to implement the JADE agents.

The same idea of enabling behaviours depending on agent type should still be possible.

\subsection{Benchmarking}

The issue with using Docker is that it makes benchmarking more difficult.
Running the entire system virtualised on one machine is not representative of a real-world deployment.

An alternative approach would be to run all the containers in a cloud provider such as Azure.
From some quick experimentation, there does not seem to be a way to measure the resource usage of a single container in Azure, just the whole container group.

Running the system on individual machines is also an option, either rented in the cloud or physical machines such as Raspberry Pis, but this would likely result in a lot of time required to set these machines up.
